function setup_RL_MPC()
    gait = 0;
    p = get_params(gait);
    initial_R_weights_unique = [p.R(1,1); p.R(2,2); p.R(3,3)]; % HAA, HFE, KFE

    % Observation space: tracking error + control effort
    obsInfo = rlNumericSpec([3 1], 'Name', 'observations');

    % Action space (3 unique weights: HAA, HFE, KFE)
    lower_bound = 0.1 * initial_R_weights_unique;
    upper_bound = 5 * initial_R_weights_unique;
    actionInfo = rlNumericSpec([3 1], ...
        'LowerLimit', lower_bound, ...
        'UpperLimit', upper_bound, ...
        'Name', 'R_weights');

    % RL environment
    env = rlFunctionEnv(obsInfo, actionInfo, @rlStepFunction, @rlResetFunction);

    % Actor network
    actorNetwork = [
        featureInputLayer(3, 'Name', 'observations')
        fullyConnectedLayer(400, 'Name', 'fc1')
        reluLayer('Name', 'relu1')
        fullyConnectedLayer(300, 'Name', 'fc2')
        reluLayer('Name', 'relu2')
        fullyConnectedLayer(3, 'Name', 'fc3')
        softplusLayer('Name', 'softplus') % ensure strictly positive outputs
        scalingLayer('Name', 'scale', 'Scale', upper_bound) % scale to action bounds
    ];

    % Initialize actor to output initial weights
    % actorNetwork(6).Bias = initial_R_weights_unique ./ upper_bound; % scaled to [0,1]
    
    % actorOpts = rlRepresentationOptions('LearnRate',1e-4,'GradientThreshold',1);
    actorOpts = rlOptimizerOptions("LearnRate",1e-3,"GradientThreshold",1);
    actor = rlDeterministicActorRepresentation(actorNetwork, ...
        obsInfo, actionInfo, ...
        'Observation', {'observations'}, ...
        'Action', {'scale'}, ...
        actorOpts);

    % Critic network (unchanged except for action size)
    statePath = [
        featureInputLayer(3, 'Name', 'observations')
        fullyConnectedLayer(400, 'Name', 'fc1')
        reluLayer('Name', 'relu1')
    ];
    actionPath = [
        featureInputLayer(3, 'Name', 'scale')
        fullyConnectedLayer(400, 'Name', 'fc2')
        reluLayer('Name', 'relu2')
    ];
    commonPath = [
        additionLayer(2,'Name','add')
        fullyConnectedLayer(300, 'Name', 'fc3')
        reluLayer('Name', 'relu3')
        fullyConnectedLayer(1, 'Name', 'Q_value')
    ];
    criticNetwork = layerGraph(statePath);
    criticNetwork = addLayers(criticNetwork, actionPath);
    criticNetwork = addLayers(criticNetwork, commonPath);
    criticNetwork = connectLayers(criticNetwork, 'relu1', 'add/in1');
    criticNetwork = connectLayers(criticNetwork, 'relu2', 'add/in2');

    % criticOpts = rlRepresentationOptions('LearnRate',1e-3,'GradientThreshold',1);
    criticOpts = rlOptimizerOptions()
    critic = rlQValueRepresentation(criticNetwork, ...
        obsInfo, actionInfo, ...
        'Observation', {'observations'}, ...
        'Action', {'scale'}, ...
        criticOpts);

    % DDPG Agent options
    agentOpts = rlDDPGAgentOptions(...
        'SampleTime',1, ...
        'TargetSmoothFactor',1e-3, ...
        'DiscountFactor',0.99, ...
        'MiniBatchSize',64, ...
        'ExperienceBufferLength',1e6);
    agentOpts.NoiseOptions.Variance = 0.01 * mean(initial_R_weights_unique); % small initial noise
    agentOpts.NoiseOptions.VarianceDecayRate = 1e-5;

    agent = rlDDPGAgent(actor, critic, agentOpts);

    % Save environment and agent
    save(fullfile(pwd, 'rlEnv_MPC_R.mat'), 'env', 'agent');
    disp('RL environment and agent saved with symmetric p.R weights.');
end
